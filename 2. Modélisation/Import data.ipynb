{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b68bda1-1a17-4e15-91c4-92a3036cfdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (2.26.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests) (1.26.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4efc86-11ce-49f6-8f6f-bc9dad8fb7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-4.6.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.9 MB)\n",
      "     |████████████████████████████████| 6.9 MB 1.7 MB/s            \n",
      "\u001b[?25hInstalling collected packages: lxml\n",
      "Successfully installed lxml-4.6.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e49424b-96fb-48d6-b91a-3728b76a1e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.1.0-py3-none-any.whl (958 kB)\n",
      "     |████████████████████████████████| 958 kB 1.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: urllib3[secure]~=1.26 in /opt/conda/lib/python3.9/site-packages (from selenium) (1.26.7)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.19.0-py3-none-any.whl (356 kB)\n",
      "     |████████████████████████████████| 356 kB 52.0 MB/s            \n",
      "\u001b[?25hCollecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.9/site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in /opt/conda/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sortedcontainers in /opt/conda/lib/python3.9/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.1)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in /opt/conda/lib/python3.9/site-packages (from urllib3[secure]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in /opt/conda/lib/python3.9/site-packages (from urllib3[secure]~=1.26->selenium) (35.0.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.9/site-packages (from urllib3[secure]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.9/site-packages (from cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (1.14.6)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.9/site-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.16.0)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
      "     |████████████████████████████████| 54 kB 12.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pycparser in /opt/conda/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (2.20)\n",
      "Installing collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed h11-0.12.0 outcome-1.1.0 selenium-4.1.0 trio-0.19.0 trio-websocket-0.9.2 wsproto-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70e8a293-1ab8-4071-9bc8-3d3938fc8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml\n",
    "import selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87cae471-3cc5-48b8-8870-08d78396825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "races = {'season': [],\n",
    "        'round': [],\n",
    "        'circuit_id': [],\n",
    "        'lat': [],\n",
    "        'long': [],\n",
    "        'country': [],\n",
    "        'date': [],\n",
    "        'url': []}\n",
    "\n",
    "for year in list(range(1990,2021)):\n",
    "    \n",
    "    url = 'https://ergast.com/api/f1/{}.json'\n",
    "    r = requests.get(url.format(year))\n",
    "    json = r.json()\n",
    "\n",
    "    for item in json['MRData']['RaceTable']['Races']:\n",
    "        try:\n",
    "            races['season'].append(int(item['season']))\n",
    "        except:\n",
    "            races['season'].append(None)\n",
    "\n",
    "        try:\n",
    "            races['round'].append(int(item['round']))\n",
    "        except:\n",
    "            races['round'].append(None)\n",
    "\n",
    "        try:\n",
    "            races['circuit_id'].append(item['Circuit']['circuitId'])\n",
    "        except:\n",
    "            races['circuit_id'].append(None)\n",
    "\n",
    "        try:\n",
    "            races['lat'].append(float(item['Circuit']['Location']['lat']))\n",
    "        except:\n",
    "            races['lat'].append(None)\n",
    "\n",
    "        try:\n",
    "            races['long'].append(float(item['Circuit']['Location']['long']))\n",
    "        except:\n",
    "            races['long'].append(None)\n",
    "\n",
    "        try:\n",
    "            races['country'].append(item['Circuit']['Location']['country'])\n",
    "        except:\n",
    "            races['country'].append(None)\n",
    "\n",
    "        try:\n",
    "            races['date'].append(item['date'])\n",
    "        except:\n",
    "            races['date'].append(None)\n",
    "\n",
    "        try:\n",
    "            races['url'].append(item['url'])\n",
    "        except:\n",
    "            races['url'].append(None)\n",
    "        \n",
    "race= pd.DataFrame(races)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f80a07d9-5c85-4213-b264-e70ccc8b0ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "race.to_csv(r'formula1\\race.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60a00683-9c2e-425d-853c-0d5eeaa5e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = []\n",
    "for year in np.array(race.season.unique()):\n",
    "    rounds.append([year, list(race[race.season == year]['round'])])\n",
    "\n",
    "# query API\n",
    "    \n",
    "results = {'season': [],\n",
    "          'round':[],\n",
    "           'circuit_id':[],\n",
    "          'driver': [],\n",
    "           'date_of_birth': [],\n",
    "           'nationality': [],\n",
    "          'constructor': [],\n",
    "          'grid': [],\n",
    "          'time': [],\n",
    "          'status': [],\n",
    "          'points': [],\n",
    "          'podium': []}\n",
    "\n",
    "for n in list(range(len(rounds))):\n",
    "    for i in rounds[n][1]:\n",
    "    \n",
    "        url = 'http://ergast.com/api/f1/{}/{}/results.json'\n",
    "        r = requests.get(url.format(rounds[n][0], i))\n",
    "        json = r.json()\n",
    "\n",
    "        for item in json['MRData']['RaceTable']['Races'][0]['Results']:\n",
    "            try:\n",
    "                results['season'].append(int(json['MRData']['RaceTable']['Races'][0]['season']))\n",
    "            except:\n",
    "                results['season'].append(None)\n",
    "\n",
    "            try:\n",
    "                results['round'].append(int(json['MRData']['RaceTable']['Races'][0]['round']))\n",
    "            except:\n",
    "                results['round'].append(None)\n",
    "\n",
    "            try:\n",
    "                results['circuit_id'].append(json['MRData']['RaceTable']['Races'][0]['Circuit']['circuitId'])\n",
    "            except:\n",
    "                results['circuit_id'].append(None)\n",
    "\n",
    "            try:\n",
    "                results['driver'].append(item['Driver']['driverId'])\n",
    "            except:\n",
    "                results['driver'].append(None)\n",
    "            \n",
    "            try:\n",
    "                results['date_of_birth'].append(item['Driver']['dateOfBirth'])\n",
    "            except:\n",
    "                results['date_of_birth'].append(None)\n",
    "                \n",
    "            try:\n",
    "                results['nationality'].append(item['Driver']['nationality'])\n",
    "            except:\n",
    "                results['nationality'].append(None)\n",
    "\n",
    "            try:\n",
    "                results['constructor'].append(item['Constructor']['constructorId'])\n",
    "            except:\n",
    "                results['constructor'].append(None)\n",
    "\n",
    "            try:\n",
    "                results['grid'].append(int(item['grid']))\n",
    "            except:\n",
    "                results['grid'].append(None)\n",
    "\n",
    "            try:\n",
    "                results['time'].append(int(item['Time']['millis']))\n",
    "            except:\n",
    "                results['time'].append(None)\n",
    "\n",
    "            try:\n",
    "                results['status'].append(item['status'])\n",
    "            except:\n",
    "                results['status'].append(None)\n",
    "\n",
    "            try:\n",
    "                results['points'].append(int(item['points']))\n",
    "            except:\n",
    "                results['points'].append(None)\n",
    "\n",
    "            try:\n",
    "                results['podium'].append(int(item['position']))\n",
    "            except:\n",
    "                results['podium'].append(None)\n",
    "\n",
    "           \n",
    "results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6677b7a-2eeb-4498-80e6-7431d88ef1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(r'results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07e2b700-8c10-4bc3-9bac-2dc7a7037352",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_standings = {'season': [],\n",
    "                    'round':[],\n",
    "                    'driver': [],\n",
    "                    'driver_points': [],\n",
    "                    'driver_wins': [],\n",
    "                   'driver_standings_pos': []}\n",
    "\n",
    "# query API\n",
    "\n",
    "for n in list(range(len(rounds))):     \n",
    "    for i in rounds[n][1]:    # iterate through rounds of each year\n",
    "    \n",
    "        url = 'https://ergast.com/api/f1/{}/{}/driverStandings.json'\n",
    "        r = requests.get(url.format(rounds[n][0], i))\n",
    "        json = r.json()\n",
    "\n",
    "        for item in json['MRData']['StandingsTable']['StandingsLists'][0]['DriverStandings']:\n",
    "            try:\n",
    "                driver_standings['season'].append(int(json['MRData']['StandingsTable']['StandingsLists'][0]['season']))\n",
    "            except:\n",
    "                driver_standings['season'].append(None)\n",
    "\n",
    "            try:\n",
    "                driver_standings['round'].append(int(json['MRData']['StandingsTable']['StandingsLists'][0]['round']))\n",
    "            except:\n",
    "                driver_standings['round'].append(None)\n",
    "                                         \n",
    "            try:\n",
    "                driver_standings['driver'].append(item['Driver']['driverId'])\n",
    "            except:\n",
    "                driver_standings['driver'].append(None)\n",
    "            \n",
    "            try:\n",
    "                driver_standings['driver_points'].append(int(item['points']))\n",
    "            except:\n",
    "                driver_standings['driver_points'].append(None)\n",
    "            \n",
    "            try:\n",
    "                driver_standings['driver_wins'].append(int(item['wins']))\n",
    "            except:\n",
    "                driver_standings['driver_wins'].append(None)\n",
    "                \n",
    "            try:\n",
    "                driver_standings['driver_standings_pos'].append(int(item['position']))\n",
    "            except:\n",
    "                driver_standings['driver_standings_pos'].append(None)\n",
    "            \n",
    "driver_standings = pd.DataFrame(driver_standings)\n",
    "\n",
    "# define lookup function to shift points and number of wins from previous rounds\n",
    "\n",
    "def lookup (df, team, points):\n",
    "    df['lookup1'] = df.season.astype(str) + df[team] + df['round'].astype(str)\n",
    "    df['lookup2'] = df.season.astype(str) + df[team] + (df['round']-1).astype(str)\n",
    "    new_df = df.merge(df[['lookup1', points]], how = 'left', left_on='lookup2',right_on='lookup1')\n",
    "    new_df.drop(['lookup1_x', 'lookup2', 'lookup1_y'], axis = 1, inplace = True)\n",
    "    new_df.rename(columns = {points+'_x': points+'_after_race', points+'_y': points}, inplace = True)\n",
    "    new_df[points].fillna(0, inplace = True)\n",
    "    return new_df\n",
    "  \n",
    "driver_standings = lookup(driver_standings, 'driver', 'driver_points')\n",
    "driver_standings = lookup(driver_standings, 'driver', 'driver_wins')\n",
    "driver_standings = lookup(driver_standings, 'driver', 'driver_standings_pos')\n",
    "\n",
    "driver_standings.drop(['driver_points_after_race', 'driver_wins_after_race', 'driver_standings_pos_after_race'], \n",
    "                      axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06c82c7c-51e2-4bb9-83f4-10b96e98411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_standings.to_csv(r'driver_standings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb51f6c4-42a8-44fd-b0c3-2f31e356b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from year 1990\n",
    "\n",
    "constructor_rounds = rounds\n",
    "\n",
    "constructor_standings = {'season': [],\n",
    "                    'round':[],\n",
    "                    'constructor': [],\n",
    "                    'constructor_points': [],\n",
    "                    'constructor_wins': [],\n",
    "                   'constructor_standings_pos': []}\n",
    "# query API\n",
    "\n",
    "for n in list(range(len(constructor_rounds))):\n",
    "    for i in constructor_rounds[n][1]:\n",
    "    \n",
    "        url = 'https://ergast.com/api/f1/{}/{}/constructorStandings.json'\n",
    "        r = requests.get(url.format(constructor_rounds[n][0], i))\n",
    "        json = r.json()\n",
    "\n",
    "        for item in json['MRData']['StandingsTable']['StandingsLists'][0]['ConstructorStandings']:\n",
    "            try:\n",
    "                constructor_standings['season'].append(int(json['MRData']['StandingsTable']['StandingsLists'][0]['season']))\n",
    "            except:\n",
    "                constructor_standings['season'].append(None)\n",
    "\n",
    "            try:\n",
    "                constructor_standings['round'].append(int(json['MRData']['StandingsTable']['StandingsLists'][0]['round']))\n",
    "            except:\n",
    "                constructor_standings['round'].append(None)\n",
    "                                         \n",
    "            try:\n",
    "                constructor_standings['constructor'].append(item['Constructor']['constructorId'])\n",
    "            except:\n",
    "                constructor_standings['constructor'].append(None)\n",
    "            \n",
    "            try:\n",
    "                constructor_standings['constructor_points'].append(int(item['points']))\n",
    "            except:\n",
    "                constructor_standings['constructor_points'].append(None)\n",
    "            \n",
    "            try:\n",
    "                constructor_standings['constructor_wins'].append(int(item['wins']))\n",
    "            except:\n",
    "                constructor_standings['constructor_wins'].append(None)\n",
    "                \n",
    "            try:\n",
    "                constructor_standings['constructor_standings_pos'].append(int(item['position']))\n",
    "            except:\n",
    "                constructor_standings['constructor_standings_pos'].append(None)\n",
    "            \n",
    "constructor_standings = pd.DataFrame(constructor_standings)\n",
    "\n",
    "constructor_standings = lookup(constructor_standings, 'constructor', 'constructor_points')\n",
    "constructor_standings = lookup(constructor_standings, 'constructor', 'constructor_wins')\n",
    "constructor_standings = lookup(constructor_standings, 'constructor', 'constructor_standings_pos')\n",
    "\n",
    "constructor_standings.drop(['constructor_points_after_race', 'constructor_wins_after_race','constructor_standings_pos_after_race' ],\n",
    "                           axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "244624d6-9fda-4f80-af5b-88aa42c45868",
   "metadata": {},
   "outputs": [],
   "source": [
    "constructor_standings.to_csv(r'constructor_standings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a4423e5-770c-4b81-9e21-9ab8b142efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "qualifying_results = pd.DataFrame()\n",
    "\n",
    "for year in list(range(1990,2021)):\n",
    "    url = 'https://www.formula1.com/en/results.html/{}/races.html'\n",
    "    r = requests.get(url.format(year))\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    # find links to all circuits for a certain year\n",
    "    \n",
    "    year_links = []\n",
    "    for page in soup.find_all('a', attrs = {'class':\"resultsarchive-filter-item-link FilterTrigger\"}):\n",
    "        link = page.get('href')\n",
    "        if f'/en/results.html/{year}/races/' in link: \n",
    "            year_links.append(link)\n",
    "    \n",
    "    # for each circuit, switch to the starting grid page and read table\n",
    "\n",
    "    year_df = pd.DataFrame()\n",
    "    new_url = 'https://www.formula1.com{}'\n",
    "    for n, link in list(enumerate(year_links)):\n",
    "        link = link.replace('race-result.html', 'starting-grid.html')\n",
    "        df = pd.read_html(new_url.format(link))\n",
    "        df = df[0]\n",
    "        df['season'] = year\n",
    "        df['round'] = n+1\n",
    "        for col in df:\n",
    "            if 'Unnamed' in col:\n",
    "                df.drop(col, axis = 1, inplace = True)\n",
    "\n",
    "        year_df = pd.concat([year_df, df])\n",
    "\n",
    "    # concatenate all tables from all years  \n",
    "        \n",
    "    qualifying_results = pd.concat([qualifying_results, year_df])\n",
    "\n",
    "# rename columns\n",
    "    \n",
    "qualifying_results.rename(columns = {'Pos': 'grid', 'Driver': 'driver_name', 'Car': 'car',\n",
    "                                     'Time': 'qualifying_time'}, inplace = True)\n",
    "# drop driver number column\n",
    "\n",
    "qualifying_results.drop('No', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df4a809b-d7f5-4b0f-8dab-4a965011fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifying_results.to_csv(r'qualifying_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77034bbc-25fb-453f-9183-dbf13a13e915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_271/4070663694.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather['weather'] = info\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "weather = race.iloc[:,[0,1,2]]\n",
    "\n",
    "info = []\n",
    "\n",
    "# read wikipedia tables\n",
    "\n",
    "for link in race.url:\n",
    "    try:\n",
    "        df = pd.read_html(link)[0]\n",
    "        if 'Weather' in list(df.iloc[:,0]):\n",
    "            n = list(df.iloc[:,0]).index('Weather')\n",
    "            info.append(df.iloc[n,1])\n",
    "        else:\n",
    "            df = pd.read_html(link)[1]\n",
    "            if 'Weather' in list(df.iloc[:,0]):\n",
    "                n = list(df.iloc[:,0]).index('Weather')\n",
    "                info.append(df.iloc[n,1])\n",
    "            else:\n",
    "                df = pd.read_html(link)[2]\n",
    "                if 'Weather' in list(df.iloc[:,0]):\n",
    "                    n = list(df.iloc[:,0]).index('Weather')\n",
    "                    info.append(df.iloc[n,1])\n",
    "                else:\n",
    "                    df = pd.read_html(link)[3]\n",
    "                    if 'Weather' in list(df.iloc[:,0]):\n",
    "                        n = list(df.iloc[:,0]).index('Weather')\n",
    "                        info.append(df.iloc[n,1])\n",
    "                    else:\n",
    "                        driver = webdriver.Chrome()\n",
    "                        driver.get(link)\n",
    "\n",
    "                        # click language button\n",
    "                        button = driver.find_element_by_link_text('Français')\n",
    "                        button.click()\n",
    "                        \n",
    "                        # find weather in italian with selenium\n",
    "                        \n",
    "                        clima = driver.find_element_by_xpath('//*[@id=\"mw-content-text\"]/div/table[1]/tbody/tr[9]/td').text\n",
    "                        info.append(clima) \n",
    "                                \n",
    "    except:\n",
    "        info.append('not found')\n",
    "\n",
    "# append column with weather information to dataframe  \n",
    "  \n",
    "weather['weather'] = info\n",
    "\n",
    "# set up a dictionary to convert weather information into keywords\n",
    "\n",
    "weather_dict = {'weather_warm': ['clair', 'clear', 'warm', 'hot', 'sunny', 'fine', 'mild', 'dégagé'],\n",
    "               'weather_cold': ['cold', 'fresh', 'chilly', 'cool'],\n",
    "               'weather_dry': ['dry', 'humide'],\n",
    "               'weather_wet': ['showers', 'wet', 'rain', 'pluvieux', 'damp', 'thunderstorms', 'rainy'],\n",
    "               'weather_cloudy': ['overcast', 'nuageux', 'clouds', 'cloudy', 'grey', 'gris']}\n",
    "\n",
    "# map new df according to weather dictionary\n",
    "\n",
    "weather_df = pd.DataFrame(columns = weather_dict.keys())\n",
    "for col in weather_df:\n",
    "    weather_df[col] = weather['weather'].map(lambda x: 1 if any(i in weather_dict[col] for i in x.lower().split()) else 0)\n",
    "   \n",
    "weather_info = pd.concat([weather, weather_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24c9fc84-b298-47fe-8679-ae5216b3edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_info.to_csv(r'weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06af103-bef4-49fd-b67d-ab7fa448fb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
